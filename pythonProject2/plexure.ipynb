{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_17436/1315092646.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\24681\\AppData\\Local\\Temp/ipykernel_17436/1315092646.py\"\u001B[1;36m, line \u001B[1;32m1\u001B[0m\n\u001B[1;33m    from pyspark.sql import\u001B[0m\n\u001B[1;37m                           ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 2 column 1 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_35824/1409895098.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"test.jsonl\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[0mtrain_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadlines\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_35824/1409895098.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"test.jsonl\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[0mtrain_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadlines\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\json\\__init__.py\u001B[0m in \u001B[0;36mloads\u001B[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[0;32m    344\u001B[0m             \u001B[0mparse_int\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mparse_float\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    345\u001B[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001B[1;32m--> 346\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_default_decoder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    347\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mcls\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    348\u001B[0m         \u001B[0mcls\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mJSONDecoder\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\json\\decoder.py\u001B[0m in \u001B[0;36mdecode\u001B[1;34m(self, s, _w)\u001B[0m\n\u001B[0;32m    335\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    336\u001B[0m         \"\"\"\n\u001B[1;32m--> 337\u001B[1;33m         \u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraw_decode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0m_w\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    338\u001B[0m         \u001B[0mend\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_w\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    339\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mend\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\json\\decoder.py\u001B[0m in \u001B[0;36mraw_decode\u001B[1;34m(self, s, idx)\u001B[0m\n\u001B[0;32m    353\u001B[0m             \u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscan_once\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    354\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 355\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mJSONDecodeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Expecting value\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    356\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mJSONDecodeError\u001B[0m: Expecting value: line 2 column 1 (char 1)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"test.jsonl\") as f:\n",
    "    train_data = [json.loads(s) for s in f.readlines()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "f=open('data.json',\"r\")\n",
    "train_data=[]\n",
    "for line in f.readlines():\n",
    "    data=json.loads(line)\n",
    "    train_data.append(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14800, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": "        type  width  height  base  radius\n0  rectangle    5.0    10.5   NaN     NaN\n1   triangle    NaN     3.0   2.0     NaN\n2     circle    NaN     NaN   NaN     4.0\n3  rectangle    5.0     5.0   NaN     NaN\n4  rectangle    5.0    10.0   NaN     NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>width</th>\n      <th>height</th>\n      <th>base</th>\n      <th>radius</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>rectangle</td>\n      <td>5.0</td>\n      <td>10.5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>triangle</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>circle</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>rectangle</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>rectangle</td>\n      <td>5.0</td>\n      <td>10.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from area_calculation import *\n",
    "df = pd.read_json(\"data.json\", lines=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total area of all shapes: 476432.28509251575\n",
      "hh 0.20201349258422852\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from area_calculation import *\n",
    "s_t = time.time()\n",
    "df = pd.read_json(\"data.json\", lines=True)\n",
    "def calculate_area(row):\n",
    "    shape_type = row['type']\n",
    "    if shape_type == 'rectangle':\n",
    "        return rectangle_area(row['width'],row['height'])\n",
    "    elif shape_type == 'triangle':\n",
    "        return triangle_area(row['base'],row['height'])\n",
    "    elif shape_type == 'circle':\n",
    "        return circle_area(row['radius'])\n",
    "    else:\n",
    "        return None  # if the shape type is unknown, return None\n",
    "df['area'] = df.apply(calculate_area, axis=1)\n",
    "df = df.dropna(subset=['area'])\n",
    "\n",
    "    # Print the total area of all shapes\n",
    "print(f\"Total area of all shapes: {df['area'].sum()}\")\n",
    "e_t = time.time()\n",
    "print('hh',e_t - s_t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_3684/2019971908.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtypes\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mFloatType\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mspark\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSparkSession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuilder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappName\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"CalculateArea\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mcalculate_area\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mshape_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdimension1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdimension2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\site-packages\\pyspark\\sql\\session.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    226\u001B[0m                             \u001B[0msparkConf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    227\u001B[0m                         \u001B[1;31m# This SparkContext may be an existing one.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 228\u001B[1;33m                         \u001B[0msc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msparkConf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    229\u001B[0m                     \u001B[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    230\u001B[0m                     \u001B[1;31m# by all sessions.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\site-packages\\pyspark\\context.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[1;34m(cls, conf)\u001B[0m\n\u001B[0;32m    382\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    383\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 384\u001B[1;33m                 \u001B[0mSparkContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconf\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    385\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\site-packages\\pyspark\\context.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[0;32m    142\u001B[0m                 \" is not allowed as it is a security risk.\")\n\u001B[0;32m    143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 144\u001B[1;33m         \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    145\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    146\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\site-packages\\pyspark\\context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[1;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[0;32m    329\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    330\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_gateway\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 331\u001B[1;33m                 \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_gateway\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgateway\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mlaunch_gateway\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    332\u001B[0m                 \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_gateway\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjvm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\site-packages\\pyspark\\java_gateway.py\u001B[0m in \u001B[0;36mlaunch_gateway\u001B[1;34m(conf, popen_kwargs)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn_info_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mException\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Java gateway process exited before sending its port number\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn_info_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"rb\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0minfo\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mException\u001B[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CalculateArea\").getOrCreate()\n",
    "\n",
    "def calculate_area(shape_type, dimension1, dimension2=None):\n",
    "    if shape_type == \"rectangle\":\n",
    "        if dimension1 and dimension2:\n",
    "            return dimension1 * dimension2\n",
    "    elif shape_type == \"triangle\":\n",
    "        if dimension1 and dimension2:\n",
    "            return 0.5 * dimension1 * dimension2\n",
    "    elif shape_type == \"circle\":\n",
    "        if dimension1:\n",
    "            return 3.14159 * dimension1 * dimension1\n",
    "    else:\n",
    "        return None  # return None if shape type is unknown or dimensions are missing\n",
    "\n",
    "udf_calculate_area = udf(calculate_area, FloatType())\n",
    "\n",
    "# Load data\n",
    "df = spark.read.json(\"data.json\")\n",
    "\n",
    "# Add area column\n",
    "df = df.withColumn(\"area\", udf_calculate_area(df.type, df.width, df.height))\n",
    "\n",
    "# Drop rows where area is None (these are the rows with invalid shape type or missing dimensions)\n",
    "df = df.na.drop(subset=[\"area\"])\n",
    "\n",
    "# Persist the DataFrame\n",
    "df.persist()\n",
    "\n",
    "# Calculate total area\n",
    "total_area = df.agg({\"area\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "print(f\"Total area: {total_area}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_3684/1418621664.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;31m# Start a Spark Session\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mspark\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSparkSession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuilder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;31m# Load JSON data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\site-packages\\pyspark\\sql\\session.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    226\u001B[0m                             \u001B[0msparkConf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    227\u001B[0m                         \u001B[1;31m# This SparkContext may be an existing one.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 228\u001B[1;33m                         \u001B[0msc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msparkConf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    229\u001B[0m                     \u001B[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    230\u001B[0m                     \u001B[1;31m# by all sessions.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\site-packages\\pyspark\\context.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[1;34m(cls, conf)\u001B[0m\n\u001B[0;32m    382\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    383\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 384\u001B[1;33m                 \u001B[0mSparkContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconf\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    385\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\site-packages\\pyspark\\context.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[0;32m    142\u001B[0m                 \" is not allowed as it is a security risk.\")\n\u001B[0;32m    143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 144\u001B[1;33m         \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    145\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    146\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\site-packages\\pyspark\\context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[1;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[0;32m    329\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    330\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_gateway\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 331\u001B[1;33m                 \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_gateway\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgateway\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mlaunch_gateway\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    332\u001B[0m                 \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_gateway\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjvm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\anaconda\\lib\\site-packages\\pyspark\\java_gateway.py\u001B[0m in \u001B[0;36mlaunch_gateway\u001B[1;34m(conf, popen_kwargs)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn_info_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mException\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Java gateway process exited before sending its port number\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn_info_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"rb\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0minfo\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mException\u001B[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "import math\n",
    "\n",
    "# Start a Spark Session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load JSON data\n",
    "df = spark.read.json(\"data.json\")\n",
    "\n",
    "# Define function to calculate area\n",
    "def calculate_area(shape_type, dimension1, dimension2=None):\n",
    "    return when(shape_type == \"rectangle\", dimension1 * dimension2).otherwise(\n",
    "        when(shape_type == \"circle\", math.pi * (dimension1/2)**2).otherwise(\n",
    "            when(shape_type == \"triangle\", 0.5 * dimension1 * dimension2).otherwise(lit(0))))\n",
    "\n",
    "# Add area column\n",
    "df = df.withColumn(\"area\", calculate_area(col(\"type\"), col(\"width\").otherwise(col(\"radius\")).otherwise(col(\"base\")), col(\"height\")))\n",
    "\n",
    "# Calculate total area\n",
    "total_area = df.selectExpr(\"sum(area)\").collect()[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = 'E:\\java\\jdk-19'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}